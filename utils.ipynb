{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "074ed543-5521-4fc5-beca-d8ee4fdd9dbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "823ff7d0-69f6-4c86-bbc4-16e2c3f3ffb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, current_timestamp\n",
    "from pyspark.sql.types import StringType\n",
    "from delta.tables import DeltaTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4223206d-1f5b-4a4d-877c-884048c41ab1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc20f43e-bcce-4fd5-a07f-ff5cfa051a65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def variables_globales() -> dict:\n",
    "    return {\n",
    "        \"container\": dbutils.secrets.get(\"scope-mbc\", \"secret-env-container\"),\n",
    "        \"storage_account\": dbutils.secrets.get(\"scope-mbc\", \"secret-env-storage-account\"),\n",
    "        \"path_base\": f\"abfss://{dbutils.secrets.get('scope-mbc', 'secret-env-container')}@{dbutils.secrets.get('scope-mbc', 'secret-env-storage-account')}.dfs.core.windows.net\" # path_base = f\"abfss://{container}@{storage_account}.dfs.core.windows.net\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c96a2d4-bc9e-4e2b-9c00-34998fd0e8a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61586622-4a7e-4609-a7ce-76e2bd9f69f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_landing(path: str) -> DataFrame:\n",
    "    path_base = variables_globales()[\"path_base\"]\n",
    "\n",
    "    df = spark.read.format(\"parquet\").load(f\"{path_base}/{path}\")\n",
    "    columns_to_cast = [col(c).cast(\"string\").alias(c) for c in df.columns]\n",
    "\n",
    "    return df.select(*columns_to_cast)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a24c730e-8bc4-4c2c-a537-0cefd539a130",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_bronze(df: DataFrame, tabla: str) -> None:\n",
    "    path_base = variables_globales()[\"path_base\"]\n",
    "    df.write.format(\"delta\").mode(\"append\").saveAsTable(tabla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16eeb0c5-fde9-4e36-ba46-35b6c1d7fb5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def merge(table_name: str, df: DataFrame, identity_column: list = []) -> None:\n",
    "    \"\"\"\n",
    "    Ejecuta un merge dinámico sobre una tabla Delta Lake utilizando las claves primarias detectadas automáticamente.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    silver_table_name : str\n",
    "        Nombre de la tabla Delta destino en formato 'schema.table'.\n",
    "    df_result : DataFrame\n",
    "        DataFrame fuente con los datos a insertar o actualizar.\n",
    "    \"\"\"\n",
    "\n",
    "    # Obtener columnas clave desde el catálogo\n",
    "    query = f\"\"\"\n",
    "    SELECT cu.column_name\n",
    "    FROM system.information_schema.key_column_usage AS cu\n",
    "    INNER JOIN system.information_schema.table_constraints AS tc\n",
    "      USING (constraint_catalog, constraint_schema, constraint_name)\n",
    "    WHERE concat_ws(\".\", cu.table_schema, cu.table_name) = '{table_name}'\n",
    "      AND tc.constraint_type = 'PRIMARY KEY'\n",
    "      AND cu.table_catalog = 'lakehouse'\n",
    "    ORDER BY ordinal_position\n",
    "    \"\"\"\n",
    "\n",
    "    df_query = spark.sql(query)\n",
    "    columns_key = [row['column_name'] for row in df_query.collect()]\n",
    "\n",
    "    # Construir condiciones de merge\n",
    "    merge_conditions = \" AND \".join([f\"m.{c} = in.{c}\" for c in columns_key])\n",
    "\n",
    "    # Cargar tabla Delta\n",
    "    delta_table = DeltaTable.forName(spark, table_name)\n",
    "    target_columns = delta_table.toDF().columns\n",
    "\n",
    "    exclusion_list_update = set(columns_key + [\"FechaAuditoriaCreacion\"])\n",
    "    exclusion_list_insert = set(identity_column)\n",
    "\n",
    "    columns_to_update = {\n",
    "        col: f\"in.{col}\" for col in target_columns if col not in exclusion_list_update\n",
    "    }\n",
    "\n",
    "    columns_to_insert = {\n",
    "        col: f\"in.{col}\" for col in target_columns if col not in exclusion_list_insert\n",
    "    }\n",
    "\n",
    "    # Ejecutar merge\n",
    "    (\n",
    "        delta_table.alias(\"m\")\n",
    "            .merge(df.alias(\"in\"), merge_conditions)\n",
    "            .whenMatchedUpdate(set=columns_to_update)\n",
    "            .whenNotMatchedInsert(values=columns_to_insert)\n",
    "            .execute()\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8915518694747741,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "utils",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
