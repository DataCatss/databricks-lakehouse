{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "074ed543-5521-4fc5-beca-d8ee4fdd9dbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "823ff7d0-69f6-4c86-bbc4-16e2c3f3ffb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from typing import Tuple, Dict\n",
    "from pyspark.sql.functions import col, current_timestamp, lit, when\n",
    "from pyspark.sql.types import StringType\n",
    "from delta.tables import DeltaTable\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4223206d-1f5b-4a4d-877c-884048c41ab1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc20f43e-bcce-4fd5-a07f-ff5cfa051a65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def variables_globales() -> dict:\n",
    "    return {\n",
    "        \"container\": dbutils.secrets.get(\"scope-mbc\", \"secret-env-container\"),\n",
    "        \"storage_account\": dbutils.secrets.get(\"scope-mbc\", \"secret-env-storage-account\"),\n",
    "        \"path_base\": f\"abfss://{dbutils.secrets.get('scope-mbc', 'secret-env-container')}@{dbutils.secrets.get('scope-mbc', 'secret-env-storage-account')}.dfs.core.windows.net\" # path_base = f\"abfss://{container}@{storage_account}.dfs.core.windows.net\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c96a2d4-bc9e-4e2b-9c00-34998fd0e8a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61586622-4a7e-4609-a7ce-76e2bd9f69f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_landing(path: str) -> DataFrame:\n",
    "    path_base = variables_globales()[\"path_base\"]\n",
    "\n",
    "    df = spark.read.format(\"parquet\").load(f\"{path_base}/{path}\")\n",
    "    columns_to_cast = [col(c).cast(\"string\").alias(c) for c in df.columns]\n",
    "\n",
    "    return df.select(*columns_to_cast)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a24c730e-8bc4-4c2c-a537-0cefd539a130",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_bronze(df: DataFrame, tabla: str) -> None:\n",
    "    path_base = variables_globales()[\"path_base\"]\n",
    "    df.write.format(\"delta\").mode(\"append\").saveAsTable(tabla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccebe63a-fdc1-47c3-b039-0cb810ad3665",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_metrics(\n",
    "    table_name: str\n",
    ") -> Tuple[Dict[str, int], str]:\n",
    "    \"\"\"\n",
    "    Extrae métricas de la última operación Delta Lake sobre una tabla (como filas afectadas y tamaño).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    table_name : str\n",
    "        Nombre de la tabla Delta.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Diccionario con métricas como:\n",
    "        - numInsertedRows\n",
    "        - numUpdatedRows\n",
    "        - numDeletedRows\n",
    "        - numOutputBytes\n",
    "    \"\"\"\n",
    "    delta_table = DeltaTable.forName(spark, table_name)\n",
    "    try:\n",
    "        history = delta_table.history()\n",
    "        for row in history.collect():\n",
    "            if row.operation == \"MERGE\":\n",
    "                metrics = row.asDict().get(\"operationMetrics\", {})\n",
    "                return {k: int(v) for k, v in metrics.items() if v.isdigit()}, table_name\n",
    "        return {}, \"\"\n",
    "    except Exception:\n",
    "        return {}, \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16eeb0c5-fde9-4e36-ba46-35b6c1d7fb5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def merge(\n",
    "    table_name: str,\n",
    "    df: DataFrame,\n",
    "    identity_column: list = [],\n",
    "    enable_delete: bool = False\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Ejecuta un merge dinámico sobre una tabla Delta Lake utilizando las claves primarias detectadas automáticamente,\n",
    "    excluyendo columnas de auditoría y devolviendo métricas para la tabla de control.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    table_name : str\n",
    "        Nombre de la tabla Delta destino en formato 'schema.table'.\n",
    "    df : DataFrame\n",
    "        DataFrame fuente con los datos a insertar o actualizar.\n",
    "    enable_delete : bool, optional\n",
    "        Si es True, se eliminan las filas que no están presentes en df. Por defecto es False.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Métricas con número de filas insertadas, actualizadas y eliminadas.\n",
    "    \"\"\"\n",
    "\n",
    "    query = f\"\"\"\n",
    "    SELECT cu.column_name\n",
    "    FROM system.information_schema.key_column_usage AS cu\n",
    "    INNER JOIN system.information_schema.table_constraints AS tc\n",
    "      USING (constraint_catalog, constraint_schema, constraint_name)\n",
    "    WHERE concat_ws(\".\", cu.table_schema, cu.table_name) = '{table_name}'\n",
    "      AND tc.constraint_type = 'PRIMARY KEY'\n",
    "      AND cu.table_catalog = 'lakehouse'\n",
    "    ORDER BY ordinal_position\n",
    "    \"\"\"\n",
    "\n",
    "    df_query = spark.sql(query)\n",
    "    columns_key = [row['column_name'] for row in df_query.collect()]\n",
    "\n",
    "    merge_conditions = \" AND \".join([f\"m.{c} = in.{c}\" for c in columns_key])\n",
    "\n",
    "    delta_table = DeltaTable.forName(spark, table_name)\n",
    "    target_columns = delta_table.toDF().columns\n",
    "\n",
    "    exclusion_list_update = set(columns_key + [\"FechaAuditoriaCreacion\"])\n",
    "    exclusion_list_insert = set(identity_column)\n",
    "\n",
    "    columns_to_update = {\n",
    "        col: f\"in.{col}\" for col in target_columns if col not in exclusion_list_update\n",
    "    }\n",
    "\n",
    "    columns_to_insert = {\n",
    "        col: f\"in.{col}\" for col in target_columns if col not in exclusion_list_insert\n",
    "    }\n",
    "\n",
    "    merge_builder = (\n",
    "        delta_table.alias(\"m\")\n",
    "            .merge(df.alias(\"in\"), merge_conditions)\n",
    "            .whenMatchedUpdate(set=columns_to_update)\n",
    "            .whenNotMatchedInsert(values=columns_to_insert)\n",
    "    )\n",
    "\n",
    "    if enable_delete:\n",
    "        merge_builder = merge_builder.whenNotMatchedBySourceDelete()\n",
    "\n",
    "    merge_builder.execute()\n",
    "\n",
    "    print(f\"Merge ejecutado en la tabla {table_name}\")\n",
    "\n",
    "    return insert_metrics(get_metrics(table_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "334245ce-fcfd-41fd-863c-97c5987bd768",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def insert_metrics(metrics_tuple: Tuple[Dict[str, int], str]) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Inserta métricas de un merge en la tabla de control.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    metrics_tuple : Tuple\n",
    "        Tupla con las métricas a insertar y el el nombre de la tabla\n",
    "    \"\"\"\n",
    "\n",
    "    metrics, table_name = metrics_tuple\n",
    "\n",
    "    job_id = dbutils.widgets.get('JobId')\n",
    "    job_run_id = dbutils.widgets.get('JobRunId')\n",
    "    task_run_id = dbutils.widgets.get('TaskRunId')\n",
    "    start_time = dbutils.widgets.get('StartTime')\n",
    "\n",
    "    table = table_name.split('.')[1]\n",
    "    layer = table_name.split('.')[0]\n",
    "    \n",
    "    df_metrics = (\n",
    "        spark.createDataFrame([metrics])\n",
    "            .withColumn('job_id', lit(int(job_id)))\n",
    "            .withColumn('job_run_id', lit(int(job_run_id)))\n",
    "            .withColumn('task_run_id', lit(int(task_run_id)))\n",
    "            .withColumn('job_start_time', lit(start_time).cast(\"timestamp\"))\n",
    "            .withColumn('job_end_time', current_timestamp())\n",
    "            .withColumn('job_duration_seconds', (col(\"job_end_time\") - col(\"job_start_time\")).cast('long'))\n",
    "            .withColumn('table', lit(table))\n",
    "            .withColumn('layer', lit(layer))\n",
    "            .withColumn('rows_in', when(col(\"layer\") != \"bronze\", lit(0)).otherwise(col('numTargetRowsInserted')))\n",
    "            .withColumn('rows_inserted', col(\"numTargetRowsInserted\"))\n",
    "            .withColumn('rows_updated', col(\"numTargetRowsUpdated\"))\n",
    "            .withColumn('rows_deleted', col(\"numTargetRowsDeleted\"))\n",
    "            .withColumn('file_bytes', col(\"numTargetBytesAdded\"))\n",
    "            .withColumn('merge_duration_seconds', col(\"executionTimeMs\") / 1000)\n",
    "            .withColumn('job_status', when(col(\"file_bytes\") > 0, lit(\"Success\")).otherwise(lit(\"Failed\")))\n",
    "    ).select(\n",
    "        col(\"job_id\"),\n",
    "        col(\"job_run_id\"),\n",
    "        col(\"task_run_id\"),\n",
    "        col(\"job_start_time\"),\n",
    "        col(\"job_end_time\"),\n",
    "        col(\"job_duration_seconds\"),\n",
    "        col(\"job_status\"),\n",
    "        col(\"table\"),\n",
    "        col(\"layer\"),\n",
    "        col(\"rows_in\"),\n",
    "        col(\"rows_inserted\"),\n",
    "        col(\"rows_updated\"),\n",
    "        col(\"rows_deleted\"),\n",
    "        col(\"file_bytes\"),\n",
    "        col(\"merge_duration_seconds\")\n",
    "    )\n",
    "\n",
    "    print(f\"Metricas insertadas en la tabla auditoria.ingestion_log\")\n",
    "\n",
    "    return df_metrics"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8915518694747741,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "utils",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
